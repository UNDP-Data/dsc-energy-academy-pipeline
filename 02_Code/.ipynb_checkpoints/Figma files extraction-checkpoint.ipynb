{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyGithub in c:\\users\\shima\\anaconda3\\lib\\site-packages (2.4.0)\n",
      "Requirement already satisfied: pynacl>=1.4.0 in c:\\users\\shima\\anaconda3\\lib\\site-packages (from PyGithub) (1.4.0)\n",
      "Requirement already satisfied: requests>=2.14.0 in c:\\users\\shima\\anaconda3\\lib\\site-packages (from PyGithub) (2.27.1)\n",
      "Requirement already satisfied: pyjwt>=2.4.0 in c:\\users\\shima\\anaconda3\\lib\\site-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (2.9.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\shima\\anaconda3\\lib\\site-packages (from PyGithub) (4.10.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in c:\\users\\shima\\anaconda3\\lib\\site-packages (from PyGithub) (1.26.9)\n",
      "Requirement already satisfied: Deprecated in c:\\users\\shima\\anaconda3\\lib\\site-packages (from PyGithub) (1.2.14)\n",
      "Requirement already satisfied: cryptography>=3.4.0 in c:\\users\\shima\\anaconda3\\lib\\site-packages (from pyjwt[crypto]>=2.4.0->PyGithub) (3.4.8)\n",
      "Requirement already satisfied: six in c:\\users\\shima\\anaconda3\\lib\\site-packages (from pynacl>=1.4.0->PyGithub) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.4.1 in c:\\users\\shima\\anaconda3\\lib\\site-packages (from pynacl>=1.4.0->PyGithub) (1.15.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shima\\anaconda3\\lib\\site-packages (from requests>=2.14.0->PyGithub) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\shima\\anaconda3\\lib\\site-packages (from requests>=2.14.0->PyGithub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shima\\anaconda3\\lib\\site-packages (from requests>=2.14.0->PyGithub) (3.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in c:\\users\\shima\\anaconda3\\lib\\site-packages (from Deprecated->PyGithub) (1.12.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shima\\anaconda3\\lib\\site-packages (from cffi>=1.4.1->pynacl>=1.4.0->PyGithub) (2.21)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.2 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install PyGithub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process multiple files separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames data has been exported to figma_frames_XN57QSSgnI4exbB2OTK1QM.csv\n",
      "Frames data has been exported to figma_frames_XN57QSSgnI4exbB2OTK1QM.json\n",
      "Frames data has been exported to figma_frames_5cH2oMyNQPZ1w9fUPM9Mfe.csv\n",
      "Frames data has been exported to figma_frames_5cH2oMyNQPZ1w9fUPM9Mfe.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "FILE_KEYS = ['XN57QSSgnI4exbB2OTK1QM', '5cH2oMyNQPZ1w9fUPM9Mfe']  # Add more file keys as needed\n",
    "\n",
    "# Figma API URL template\n",
    "FIGMA_API_URL_TEMPLATE = 'https://api.figma.com/v1/files/{}'\n",
    "\n",
    "# Headers for the API request\n",
    "headers = {\n",
    "    'X-Figma-Token': API_TOKEN\n",
    "}\n",
    "\n",
    "# Function to determine the appropriate header tag based on font size\n",
    "def determine_header_tag(font_size):\n",
    "    if font_size >= 32:\n",
    "        return 'h1'\n",
    "    elif 24 <= font_size < 32:\n",
    "        return 'h2'\n",
    "    elif 20 <= font_size < 24:\n",
    "        return 'h3'\n",
    "    elif 16 <= font_size < 20:\n",
    "        return 'h4'\n",
    "    elif 14 <= font_size < 16:\n",
    "        return 'h5'\n",
    "    else:\n",
    "        return 'h6'\n",
    "\n",
    "# Function to determine the template type based on frame content\n",
    "def determine_template_type(frame):\n",
    "    contains_text = any(node['type'] == 'TEXT' for node in frame['children'])\n",
    "    contains_image = any(node['type'] == 'IMAGE' for node in frame['children'])\n",
    "\n",
    "    if contains_text and not contains_image:\n",
    "        return 'Text'\n",
    "    elif contains_image and not contains_text:\n",
    "        return 'Photo'\n",
    "    else:\n",
    "        return 'Photo'\n",
    "\n",
    "# Function to process a frame and extract relevant information\n",
    "def process_frame(frame, section_number, subsection_number=None):\n",
    "    # Check if 'absoluteBoundingBox' exists\n",
    "    if 'absoluteBoundingBox' not in frame:\n",
    "        print(f\"Warning: 'absoluteBoundingBox' missing for frame '{frame.get('name', 'Unnamed')}'. Skipping this frame.\")\n",
    "        return None  # Return None to indicate skipping this frame\n",
    "    \n",
    "    template_type = determine_template_type(frame)\n",
    "    image_id = ', '.join([node['id'] for node in frame['children'] if node['type'] == 'IMAGE'])\n",
    "    \n",
    "    frame_data = {\n",
    "        'Section': section_number,\n",
    "        'Subsection': subsection_number,  # New field for subsections\n",
    "        'Section Title': frame['name'],  # Keep section title as is\n",
    "        'Template Type': template_type,\n",
    "        'Column': 0,  # Placeholder, will be set after sorting\n",
    "        'Text': '',  # Store the concatenated text with header tags\n",
    "        'id': image_id,  # Original id\n",
    "        'x': frame['absoluteBoundingBox']['x'],  # Capture the x position for sorting\n",
    "        'y': frame['absoluteBoundingBox']['y']   # Capture the y position for sorting\n",
    "    }\n",
    "\n",
    "    for node in frame['children']:\n",
    "        if node['type'] == 'TEXT':\n",
    "            font_size = node.get('style', {}).get('fontSize', 14)  # Default to 14 if no fontSize is found\n",
    "            header_tag = determine_header_tag(font_size)\n",
    "            text = node['characters']\n",
    "            # Concatenate text with the appropriate header tags\n",
    "            frame_data['Text'] += f'<{header_tag}>{text}</{header_tag}>'\n",
    "    \n",
    "    return frame_data\n",
    "\n",
    "# Function to recursively extract frames from the file data\n",
    "def extract_frames(nodes, section_number=1, subsection_number=None):\n",
    "    frames = []  # Initialize a new list for each call to avoid data mix-up\n",
    "    for node in nodes:\n",
    "        if node['type'] == 'FRAME':\n",
    "            if subsection_number is None:\n",
    "                processed_frame = process_frame(node, section_number)\n",
    "                if processed_frame:  # Only add if the frame was successfully processed\n",
    "                    frames.append(processed_frame)\n",
    "                    section_number += 1\n",
    "            else:\n",
    "                processed_frame = process_frame(node, section_number, subsection_number)\n",
    "                if processed_frame:\n",
    "                    frames.append(processed_frame)\n",
    "                    subsection_number += 1\n",
    "        if 'children' in node:\n",
    "            frames.extend(extract_frames(node['children'], section_number, subsection_number))  # Collect all child frames\n",
    "    return frames\n",
    "\n",
    "# Function to save the frames data to a CSV file\n",
    "def save_to_csv(frames, filename):\n",
    "    # Assign the correct column number after sorting\n",
    "    for i, frame in enumerate(frames):\n",
    "        frame['Column'] = i + 1\n",
    "        frame.pop('x', None)  # Remove 'x' after sorting\n",
    "        frame.pop('y', None)  # Remove 'y' after sorting\n",
    "\n",
    "    # Specify the CSV column headers\n",
    "    csv_columns = ['Section', 'Subsection', 'Section Title', 'Template Type', 'Column', 'Text', 'id']\n",
    "    \n",
    "    # Define the path to save the CSV file in the \"01_Results\" folder\n",
    "    results_folder = os.path.join('..', '01_Results')\n",
    "    os.makedirs(results_folder, exist_ok=True)  # Ensure the folder exists\n",
    "\n",
    "    # Full path to the CSV file\n",
    "    csv_path = os.path.join(results_folder, filename)\n",
    "    \n",
    "    # Open the CSV file for writing\n",
    "    with open(csv_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Write each frame's data to the CSV file\n",
    "        for frame in frames:\n",
    "            writer.writerow(frame)\n",
    "\n",
    "# Function to save the frames data to JSON files\n",
    "def save_to_json(frames, filename):\n",
    "    # Assign the correct column number after sorting\n",
    "    for i, frame in enumerate(frames):\n",
    "        frame['Column'] = i + 1\n",
    "        frame.pop('x', None)  # Remove 'x' after sorting\n",
    "        frame.pop('y', None)  # Remove 'y' after sorting\n",
    "    \n",
    "    # Define the path to save the JSON file in the \"00_API\" folder\n",
    "    api_folder = os.path.join('..', '00_API')\n",
    "    os.makedirs(api_folder, exist_ok=True)  # Ensure the folder exists\n",
    "\n",
    "    # Full path to the JSON file\n",
    "    json_path = os.path.join(api_folder, filename)                   \n",
    "\n",
    "    # Convert the frames list to a JSON structure and save to file\n",
    "    with open(json_path, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(frames, json_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Main processing function to handle multiple files\n",
    "def process_figma_files(file_keys):\n",
    "    for file_key in file_keys:\n",
    "        response = requests.get(FIGMA_API_URL_TEMPLATE.format(file_key), headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            file_data = response.json()\n",
    "            if 'document' in file_data:\n",
    "                # Extract frames for the current file\n",
    "                frames = extract_frames(file_data['document']['children'])\n",
    "                \n",
    "                # Sort frames by their position on the canvas (first by y, then by x)\n",
    "                frames.sort(key=lambda f: (f.get('y', float('inf')), f.get('x', float('inf'))))\n",
    "                \n",
    "                # Generate unique filenames for each file\n",
    "                csv_filename = f'figma_frames_{file_key}.csv'\n",
    "                json_filename = f'figma_frames_{file_key}.json'\n",
    "                \n",
    "                # Export the frames data to a CSV file\n",
    "                save_to_csv(frames, csv_filename)\n",
    "                print(f\"Frames data has been exported to {csv_filename}\")\n",
    "\n",
    "                # Export the frames data to a JSON file\n",
    "                save_to_json(frames, json_filename)\n",
    "                print(f\"Frames data has been exported to {json_filename}\")\n",
    "            else:\n",
    "                print(f\"Warning: 'document' key not found in file with key {file_key}. Skipping this file.\")\n",
    "        else:\n",
    "            print(f\"Error: Failed to fetch data for file key {file_key}. Status code: {response.status_code}\")\n",
    "            print(f\"Response: {response.text}\")\n",
    "\n",
    "# Execute the main function with the list of file keys\n",
    "process_figma_files(FILE_KEYS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### making the code run in sequencce not parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully accessed repository: UNDP-Data/dsc-energy-academy-pipeline\n",
      "Frames data has been exported to figma_frames_XN57QSSgnI4exbB2OTK1QM.csv\n",
      "Frames data has been exported to figma_frames_XN57QSSgnI4exbB2OTK1QM.json\n",
      "Frames data has been exported to figma_frames_5cH2oMyNQPZ1w9fUPM9Mfe.csv\n",
      "Frames data has been exported to figma_frames_5cH2oMyNQPZ1w9fUPM9Mfe.json\n"
     ]
    },
    {
     "ename": "UnknownObjectException",
     "evalue": "404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/contents#create-or-update-file-contents\", \"status\": \"404\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnknownObjectException\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mupload_to_github\u001b[1;34m(local_file_path, repo_folder_path, commit_message)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# Check if the file already exists in the repository\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     existing_file \u001b[38;5;241m=\u001b[39m \u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_contents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgithub_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# If it exists, update it\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\github\\Repository.py:2204\u001b[0m, in \u001b[0;36mRepository.get_contents\u001b[1;34m(self, path, ref)\u001b[0m\n\u001b[0;32m   2203\u001b[0m     url_parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mref\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ref\n\u001b[1;32m-> 2204\u001b[0m headers, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequestJsonAndCheck\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2205\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2206\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/contents/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2207\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2208\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2210\u001b[0m \u001b[38;5;66;03m# Handle 302 redirect response\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\github\\Requester.py:550\u001b[0m, in \u001b[0;36mRequester.requestJsonAndCheck\u001b[1;34m(self, verb, url, parameters, headers, input)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequestJsonAndCheck\u001b[39m(\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    544\u001b[0m     verb: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28minput\u001b[39m: Optional[Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    549\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Any]:\n\u001b[1;32m--> 550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__check\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequestJson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__customConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\github\\Requester.py:611\u001b[0m, in \u001b[0;36mRequester.__check\u001b[1;34m(self, status, responseHeaders, output)\u001b[0m\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m--> 611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreateException(status, responseHeaders, data)\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m responseHeaders, data\n",
      "\u001b[1;31mUnknownObjectException\u001b[0m: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/contents#get-repository-content\", \"status\": \"404\"}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnknownObjectException\u001b[0m                    Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 57>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     54\u001b[0m FILE_KEYS \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXN57QSSgnI4exbB2OTK1QM\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5cH2oMyNQPZ1w9fUPM9Mfe\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Replace with your list of file keys\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Execute the full pipeline\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFILE_KEYS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mrun_pipeline\u001b[1;34m(file_keys)\u001b[0m\n\u001b[0;32m     45\u001b[0m     json_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m00_API\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigma_frames_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m))    \u001b[38;5;66;03m# Path to the local JSON file\u001b[39;00m\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;66;03m# Upload the generated CSV and JSON files to GitHub\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m     \u001b[43mupload_to_github\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m01_Results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAdd CSV file for Figma file \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mfile_key\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     upload_to_github(json_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m00_API\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdd JSON file for Figma file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline complete. Files uploaded to GitHub.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mupload_to_github\u001b[1;34m(local_file_path, repo_folder_path, commit_message)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpdated file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgithub_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# If the file doesn't exist, create a new one\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m     \u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgithub_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated new file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgithub_file_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\github\\Repository.py:2457\u001b[0m, in \u001b[0;36mRepository.create_file\u001b[1;34m(self, path, message, content, branch, committer, author)\u001b[0m\n\u001b[0;32m   2454\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_defined(committer):\n\u001b[0;32m   2455\u001b[0m     put_parameters[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommitter\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m committer\u001b[38;5;241m.\u001b[39m_identity\n\u001b[1;32m-> 2457\u001b[0m headers, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requester\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequestJsonAndCheck\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2458\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPUT\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2459\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murl\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/contents/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquote\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2460\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mput_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2461\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m   2464\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: github\u001b[38;5;241m.\u001b[39mContentFile\u001b[38;5;241m.\u001b[39mContentFile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requester, headers, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m], completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   2465\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommit\u001b[39m\u001b[38;5;124m\"\u001b[39m: github\u001b[38;5;241m.\u001b[39mCommit\u001b[38;5;241m.\u001b[39mCommit(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requester, headers, data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommit\u001b[39m\u001b[38;5;124m\"\u001b[39m], completed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m   2466\u001b[0m }\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\github\\Requester.py:550\u001b[0m, in \u001b[0;36mRequester.requestJsonAndCheck\u001b[1;34m(self, verb, url, parameters, headers, input)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequestJsonAndCheck\u001b[39m(\n\u001b[0;32m    543\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    544\u001b[0m     verb: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28minput\u001b[39m: Optional[Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    549\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Any]:\n\u001b[1;32m--> 550\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__check\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequestJson\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__customConnection\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\github\\Requester.py:611\u001b[0m, in \u001b[0;36mRequester.__check\u001b[1;34m(self, status, responseHeaders, output)\u001b[0m\n\u001b[0;32m    609\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__structuredFromJson(output)\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[1;32m--> 611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreateException(status, responseHeaders, data)\n\u001b[0;32m    612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m responseHeaders, data\n",
      "\u001b[1;31mUnknownObjectException\u001b[0m: 404 {\"message\": \"Not Found\", \"documentation_url\": \"https://docs.github.com/rest/repos/contents#create-or-update-file-contents\", \"status\": \"404\"}"
     ]
    }
   ],
   "source": [
    "# Step 1: Initialize GitHub Connection\n",
    "import os\n",
    "from github import Github\n",
    "\n",
    "# GitHub repository information\n",
    "\n",
    "GITHUB_REPO = 'UNDP-Data/dsc-energy-academy-pipeline'  # Replace with your GitHub repository in the form 'username/repository'\n",
    "\n",
    "# Initialize GitHub object\n",
    "# g = Github(GITHUB_TOKEN)\n",
    "\n",
    "try:\n",
    "#     repo = g.get_repo(GITHUB_REPO)\n",
    "    print(f\"Successfully accessed repository: {repo.full_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error accessing repository: {e}\")\n",
    "\n",
    "# Function to upload file to GitHub in the specified folder\n",
    "def upload_to_github(local_file_path, repo_folder_path, commit_message):\n",
    "    with open(local_file_path, 'rb') as file:\n",
    "        content = file.read()\n",
    "    file_name = os.path.basename(local_file_path)\n",
    "    github_file_path = f\"{repo_folder_path}/{file_name}\"\n",
    "\n",
    "    try:\n",
    "        # Check if the file already exists in the repository\n",
    "        existing_file = repo.get_contents(github_file_path)\n",
    "        # If it exists, update it\n",
    "        repo.update_file(existing_file.path, commit_message, content, existing_file.sha)\n",
    "        print(f\"Updated file: {github_file_path}\")\n",
    "    except Exception:\n",
    "        # If the file doesn't exist, create a new one\n",
    "        repo.create_file(github_file_path, commit_message, content)\n",
    "        print(f\"Created new file: {github_file_path}\")\n",
    "\n",
    "# Function to run the full pipeline\n",
    "def run_pipeline(file_keys):\n",
    "    # Step 1: Process Figma files and generate CSV/JSON files (Already implemented in your previous code)\n",
    "    process_figma_files(file_keys)\n",
    "\n",
    "    # Step 2: Upload the generated CSV and JSON files to GitHub\n",
    "    for file_key in file_keys:\n",
    "        # Define the paths for the local files and corresponding GitHub folders\n",
    "        csv_file = os.path.abspath(os.path.join('..', '01_Results', f'figma_frames_{file_key}.csv'))  # Path to the local CSV file\n",
    "        json_file = os.path.abspath(os.path.join('..', '00_API', f'figma_frames_{file_key}.json'))    # Path to the local JSON file\n",
    "        \n",
    "        # Upload the generated CSV and JSON files to GitHub\n",
    "        upload_to_github(csv_file, '01_Results', f\"Add CSV file for Figma file {file_key}\")\n",
    "        upload_to_github(json_file, '00_API', f\"Add JSON file for Figma file {file_key}\")\n",
    "\n",
    "    print(\"Pipeline complete. Files uploaded to GitHub.\")\n",
    "\n",
    "# List of Figma file keys for which files were generated\n",
    "FILE_KEYS = ['XN57QSSgnI4exbB2OTK1QM', '5cH2oMyNQPZ1w9fUPM9Mfe']  # Replace with your list of file keys\n",
    "\n",
    "# Execute the full pipeline\n",
    "run_pipeline(FILE_KEYS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
